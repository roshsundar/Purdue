{"cells":[{"cell_type":"markdown","metadata":{"id":"zmwWs4S9cRIn"},"source":["# ECE 570 Assignment 9 Exercise\n","\n","Your Name: Roshan Sundar"]},{"cell_type":"markdown","metadata":{"id":"WfirXJR3qvB0"},"source":["For this assignment, you will explore various density estimation methods."]},{"cell_type":"markdown","metadata":{"id":"Vmlsa88cqmib"},"source":["## Exercise 1: Density estimation in 1D (60/100 points)\n","In this exercise, you will write code to estimate 1D densities.\n","Specifically, you will write code to estimate a Gaussian density, a histogram density, and a kernel density."]},{"cell_type":"markdown","metadata":{"id":"Vcvio-cPoMLw"},"source":["### Task 1.1: Gaussian density (20/100 points)\n","For this first one you will estimate a Gaussian density via MLE.\n","As discussed in class, this simplifies to estimating the mean and standard deviation of the data and using these empirical estimates for the Gaussian distribution.\n","The Gaussian PDF can be evaluated using the function [`scipy.stats.norm.pdf`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html). Do not change the numpy random seed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qwdHcFVkoMLx"},"outputs":[],"source":["import numpy as np\n","import scipy.stats\n","from sklearn.base import BaseEstimator\n","np.random.seed(42)\n","class GaussianDensity(BaseEstimator):\n","    def fit(self, X, y=None):\n","        ##### Your code here #####\n","        # You should estimate the mean and std of the data  and save as self.mean_ and self.std_\n","        # (note that X will be shape (n,1) because there is only 1 feature).\n","\n","\n","        ##########################\n","        return self\n","\n","    def predict_proba(self, X):\n","        ##### Your code here #####\n","        # This should return the PDF values for each sample in X (again of shape (n, 1))\n","        # This should use your self.mean_ and self.std_ variables saved from the fit method\n","\n","\n","        return pdf_values  # Output should be of shape (n,), i.e., a 1D array\n","        ##########################"]},{"cell_type":"markdown","metadata":{"id":"-r8Hops7oMLx"},"source":["### Task 1.2: Histogram density (20/100 points)\n","Now you will implement a histogram density estimate given min, max and number of bins.\n","The function [`np.searchsorted`](https://numpy.org/doc/stable/reference/generated/numpy.searchsorted.html) may be useful but is not required. Additional instructions are inline in the code template below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fp_hCX40oMLy"},"outputs":[],"source":["import numpy as np\n","import scipy.stats\n","from sklearn.base import BaseEstimator\n","np.random.seed(42)\n","class HistogramDensity(BaseEstimator):\n","    def __init__(self, n_bins, min_val, max_val):\n","        self.n_bins = n_bins\n","        self.min_val = min_val\n","        self.max_val = max_val\n","\n","    def fit(self, X, y=None):\n","        ##### Your code here #####\n","        # First create equally spaced bin_edges based on min_val, max_val and n_bins\n","        #  and save as self.bin_edges_\n","        #  (note the shape of self.bin_edges_ should be (n_bins+1,) )\n","        # Second, estimate the frequency for each bin based on the input data X\n","        #  (i.e., the number of training samples that fall into that bin divided\n","        #  by the total number of samples)\n","        # Third, using the probability for each bin, compute the density value (i.e., PDF) for\n","        #  each bin. (Note you will have to account for the width of the bin to ensure\n","        #  that integrating your density function from min_value to max_value will be 1).\n","        #  Save the density per bin as self.pdf_per_bin_ which should have the shape (n_bins,)\n","\n","        ##########################\n","        return self\n","\n","    def predict_proba(self, X):\n","        ##### Your code here #####\n","        # You should return the PDF value of the samples X.  This requires finding out which\n","        #  bin each sample falls into and returning it's corresponding density value\n","        #  **Importantly, if the value is less than min_value or greater than max_value,\n","        #    then a pdf value of 0 should be returned.\n","\n","\n","        return pdf_values  # Output should be of shape (n,), i.e., a 1D array\n","        ##########################"]},{"cell_type":"markdown","metadata":{"id":"7NB1YGHBoMLz"},"source":["### Task 1.3: Kernel density (20/100 points)\n","Now you will implement a kernel density estimate (KDE) via a Gaussian kernel given the bandwidth parameter (i.e., the standard deviation of the Gaussian kernel.\n","Specifically, the Gaussian kernel density is given by:\n","$$p(x; \\mathcal{D}) = \\frac{1}{n}\\sum_{i=1}^n p_{\\mathcal{N}}(x; \\mu = x_i, \\sigma=h) $$\n","where $\\mathcal{D}=\\{x_i\\}_{i=1}^n$ is a training dataset of $n$ samples, $p_{\\mathcal{N}}$ is the Gaussian/normal density function and $h$ is called the bandwidth hyperparameter of the KDE model.\n","(Note that fitting merely requires saving the training dataset. The saved training data is then used at test time to compute the densities of new samples.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SiLBy4yQoMLz"},"outputs":[],"source":["import numpy as np\n","import scipy.stats\n","from sklearn.base import BaseEstimator\n","np.random.seed(42)\n","class KernelDensity(BaseEstimator):\n","    def __init__(self, bandwidth):\n","        self.bandwidth = bandwidth\n","\n","    def fit(self, X, y=None):\n","        ##### Your code here #####\n","        # Save the training data in self.X_train_\n","\n","        ##########################\n","        return self\n","\n","    def predict_proba(self, X):\n","        ##### Your code here #####\n","        # You should return the KDE PDF value of the samples X.\n","        #  Note that the mean above is over the TRAINING samples, not the test samples\n","        #  so you should use the samples saved by the fit method.\n","\n","        return pdf_values  # Output should be of shape (n,), i.e., a 1D array\n","        ##########################"]},{"cell_type":"markdown","metadata":{"id":"JEg-fH8tPRRW"},"source":["You must run the testing code below for your density estimators."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KUtza5gyoML0","scrolled":false},"outputs":[],"source":["# %pdb on\n","import scipy.stats\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","# Generate some data and split into train and test\n","np.random.seed(42) # Fix random seed\n","min_val, max_val = -5, 5\n","diff = max_val - min_val\n","X = diff * np.vstack([scipy.stats.beta(6,1).rvs(size=(300,1)), scipy.stats.beta(2,7).rvs(size=(100,1))]) - diff/2\n","X_train, X_test = train_test_split(X, test_size=0.5, random_state=15)\n","print(X_train.shape, X_test.shape)\n","\n","# Loop through models\n","models = [GaussianDensity(),\n","          HistogramDensity(n_bins=15, min_val=min_val, max_val=max_val),\n","          KernelDensity(bandwidth=1)\n","         ]\n","for model in models:\n","    print(f'Fitting {type(model).__name__} model')\n","    # Fit models\n","    model.fit(X_train)\n","\n","    # Sanity checks\n","    xq = np.linspace(min_val-diff, max_val+diff, num=1000)\n","    pdf_vals = model.predict_proba(xq.reshape(-1, 1))\n","    # Check that right size and >= 0\n","    print(f'{len(pdf_vals.shape) == 1 and pdf_vals.shape[0] == len(xq)}, Shape={pdf_vals.shape}'\n","          f' - Is the output the correct shape?')\n","    print(f'{np.all(pdf_vals>=0)}, Num neg={np.sum(pdf_vals < 0)} - Are all pdf values >= 0? ')\n","\n","    # Check that integrates to 1 vai approximate numerical integration\n","    model_pdf = lambda x: model.predict_proba(np.array(x).reshape(1,1))[0]\n","    quad_out = scipy.integrate.quad(model_pdf, min_val - diff, max_val + diff, limit=100, full_output=True)\n","    # print(f'{np.abs(quad_out[0] - 1) < 1e-4}, quad_out={quad_out[0]} - Does the PDF integrate to 1? ')\n","    print(f'quad_out={quad_out[0]}')\n","    print('')\n","\n","    # Plot density model\n","    plt.plot(xq, pdf_vals, label=type(model).__name__)\n","\n","plt.legend()"]},{"cell_type":"markdown","metadata":{"id":"FMvBZk_DoML0"},"source":["## Exercise 2: Determine optimal hyperparameters based on 10-fold cross validation (40/100 points)\n","In this exercise, you need to write code that will use your estimators from above to automatically choose the best hyperparameters for the histogram and kernel density estimator.  In particular, find the best `n_bins` and `bandwidth` for the histogram and KDE respectively.\n"]},{"cell_type":"markdown","source":["### Task 1: Implement custom scorer function for use in GridSearchCV (20/100 points)\n","To do this, you will need to implement a `scorer` function that will compute the log likelihood of the data given (higher is better).\n","This function takes in the model, the input data X and y_true (which defaults to None since this is an unsupervised problem and can be ignored).\n","\n","Since we are computing the log of probabilities, we have to be careful on the case where the probability for a certain sample is zero, since the log(0) is negative infinity. And this phenomenon can happen when we use more and more bins to approximate the density with Histogram density model(Consider the case where the original density value is small for a certain range of x, and when we do sampling on the distribution, there is a high likelihood that none of the sampled points fall into that range, i.e the probability bin will have 0 height on that range).\n","\n","One easy way to overcome this issue is to add a small number epsilon (e.g 1e-15) on the probability value that is 0. The code might look like this:\n","`pdf_vector[pdf_vector < lam] = lam # where lam is a small value like 1e-15`"],"metadata":{"id":"bNsiR5cUM46G"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z-EGjl_NoML0"},"outputs":[],"source":["def mean_log_likelihood_scorer(model, X, y_true=None, lam=1e-15):\n","    ########## Your code here ########\n","    # Compute and return the mean log probability of the data\n","    #  (Note y_true is not used)\n","\n","\n","\n","    ##########################\n","    return mean_log_val"]},{"cell_type":"markdown","metadata":{"id":"JEDbzx2PoML1"},"source":["### Task 2: Estimate best hyperparameters (20/100 points)\n","Now you need to implement the `estimate_param` function. It takes in the density model, train and test dataset, parameter searching grid, model evaluation function and the number of folds for cross validaiton, and outputs the grid search result and the score on the test dataset. It uses sklearn's cross validation utilities to cross validate using the training data to determine the best parameters. You should implement grid search on the train dataset to get the model with the best parameter (note for `scoring` argument, you just pass `score_function` directly without the parenthesis; this is known as passing a function to another function) and then calculate the score on the test dataset based on the best model.\n","\n","After implementing the `estimate_param` function, you should call the function with the correct inputs.\n","\n","For the `score_function` argument, you need to use the `mean_log_likelihood_scorer`.\n","\n","For this part, you want to estimate `n_bins` for `HistogramDensity`. You should try 2-20 number of bins.\n","\n","You should use 10 fold cross validation. Extract `n_bins` from the grid search results as the `best_n_bins`.\n","\n","Finally, print out the optimal hyperparameters and, using the optimal hyperparameters, print out the log likelihood of the test data for both the histogram and KDE model.\n","\n","The expected output for $n\\_bins$ estimation should be (you need to get the same result to get full credits):\n","\n","~~~\n","The best parameter given for n_bins is 7\n","Log-likelihood for test data is -1.886976453776378\n","~~~"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vmDKahkoML1"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","\n","def estimate_param(X_train, X_test, density_model, param_grid, score_function, cv):\n","  \"\"\"\n","  Complete this function by using the GridSearchCV to search for the best parameter within the grid\n","  Inputs:\n","    X_train: training data\n","    X_test: testing data\n","    density_model: the density estimation function\n","    param_grid: a dictionary of the searching grid\n","    score_function: a function that evaluates the model on a dataset\n","    cv: number of folds for cross validation\n","  Output:\n","    grid_search_cv: the estimator after fitting on the training data\n","    test_log_likelihood: the log-likelihood of test set using the best number of bins\n","  \"\"\"\n","  ############ Your code here #############\n","  grid_search_cv =\n","\n","  test_log_likelihood =\n","  #########################################\n","  return grid_search_cv, test_log_likelihood\n","\n","# Call the estimation function with the desired parameters\n","#  and extract the best number of bins selected by CV\n","np.random.seed(42) # Fix random seed\n","\n","############ Your code here #############\n","\n","#########################################\n","\n","print(f\"The best parameter given for n_bins is {best_n_bins}\")\n","print(f\"Log-likelihood for test data is {test_log_likelihood}\")"]},{"cell_type":"markdown","source":["For this part, you want to estimate `bandwidth` for `KernelDensity`. You should try 50 bandwidth parameters linearly spaced between 0.1 and 10.\n","\n","You should use 10 fold cross validation. Extract `bandwidth` from the grid search results as the `best_bandwidth`.\n","\n","The expected output for $bandwidth$ estimation should be  (you need to get the same result to get full credits):\n","\n","~~~\n","The best parameter given for bandwidth is 0.3020408163265306\n","Log-likelihood for test data is -1.9436632867557484\n","~~~"],"metadata":{"id":"M3sMiuN5yc1f"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vsIkeYDXuVyj"},"outputs":[],"source":["# Call the estimation function with the desired parameters\n","#  and extract the best bandwidth as selected by CV\n","np.random.seed(42) # Fix random seed\n","############ Your code here #############\n","\n","#########################################\n","\n","print(f\"The best parameter given for bandwidth is {best_bandwidth}\")\n","print(f\"Log-likelihood for test data is {test_log_likelihood}\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}}},"nbformat":4,"nbformat_minor":0}